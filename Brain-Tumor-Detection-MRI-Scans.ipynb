{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumor Detection using VGG-16 Convolutional Neural Network (CNN) with Transfer Learning\n",
    "Aviral Chharia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim\n",
    "\n",
    "Aim of the project is to use Computer Vision techniques of Deep Learning to correctly identify & map Brain Tumor for assistance in Robotic Surgery. In this part, we build a CNN model that would classify if subject has a tumor or not based on MRI scan. We have used VGG-16 model architecture and weights to train the model for this binary classification problem. `Accuracy` has been used as a metric to justify the model performance which can be defined as:\n",
    "\n",
    "$\\textrm{Accuracy} = \\frac{\\textrm{Number of correclty predicted images}}{\\textrm{Total number of tested images}} \\times 100\\%$\n",
    "\n",
    "\n",
    "# Dataset\n",
    "\n",
    "It conists of MRI scans of two classes:\n",
    "\n",
    "`NO` - no tumor, encoded as `0`\n",
    "\n",
    "`YES` - tumor, encoded as `1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing Relevant Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": true
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  \n",
    "import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os \n",
    "import numpy as np \n",
    "\n",
    "\n",
    "import shutil import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt from sklearn.preprocessing \n",
    "import LabelBinarizer from sklearn.model_selection \n",
    "import train_test_split from sklearn.metrics \n",
    "import accuracy_score, confusion_matrix \n",
    "import plotly.graph_objs as go from plotly.offline \n",
    "import init_notebook_mode, iplot from plotly \n",
    "import tools from keras.preprocessing.image \n",
    "import ImageDataGenerator from keras.applications.vgg16 \n",
    "import VGG16, preprocess_input from keras \n",
    "import layers from keras.models \n",
    "import Model, Sequential from tensorflow.keras.optimizers \n",
    "import Adam from keras.callbacks \n",
    "import EarlyStopping init_notebook_mode(connected=True) RANDOM_SEED = 123\n",
    "\n",
    "!pip install imutils\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly import tools\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras import layers\n",
    "from keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "init_notebook_mode(connected=True)\n",
    "RANDOM_SEED = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Test-Train Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into `train`, `val` and `test` folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": true
   },
   "source": [
    "!apt-get install tree\n",
    "clear_output()\n",
    "# create new folders\n",
    "!mkdir TRAIN TEST VAL TRAIN/YES TRAIN/NO TEST/YES TEST/NO VAL/YES VAL/NO\n",
    "!tree -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMG_PATH = 'data/'\n",
    "# split the data by train/val/test\n",
    "for CLASS in os.listdir(IMG_PATH):\n",
    "    if not CLASS.startswith('.'):\n",
    "        IMG_NUM = len(os.listdir(IMG_PATH + CLASS))\n",
    "        for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + CLASS)):\n",
    "            img = IMG_PATH + CLASS + '/' + FILE_NAME\n",
    "            if n < 5:\n",
    "                shutil.copy(img, 'TEST/' + CLASS.upper() + '/' + FILE_NAME)\n",
    "            elif n < 0.8*IMG_NUM:\n",
    "                shutil.copy(img, 'TRAIN/'+ CLASS.upper() + '/' + FILE_NAME)\n",
    "            else:\n",
    "                shutil.copy(img, 'VAL/'+ CLASS.upper() + '/' + FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Import & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "def load_data(dir_path, img_size=(100,100)):\n",
    "    \"\"\"\n",
    "    Load resized images as np.arrays to workspace\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    i = 0\n",
    "    labels = dict()\n",
    "    for path in tqdm(sorted(os.listdir(dir_path))):\n",
    "        if not path.startswith('.'):\n",
    "            labels[i] = path\n",
    "            for file in os.listdir(dir_path + path):\n",
    "                if not file.startswith('.'):\n",
    "                    img = cv2.imread(dir_path + path + '/' + file)\n",
    "                    X.append(img)\n",
    "                    y.append(i)\n",
    "            i += 1\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    print(f'{len(X)} images loaded from {dir_path} directory.')\n",
    "    return X, y, labels\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (6,6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    cm = np.round(cm,2)\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1208/1024847647.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# use predefined function to load the image data into workspace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEST_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVAL_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = 'TRAIN/'\n",
    "TEST_DIR = 'TEST/'\n",
    "VAL_DIR = 'VAL/'\n",
    "IMG_SIZE = (224,224)\n",
    "\n",
    "# use predefined function to load the image data into workspace\n",
    "X_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\n",
    "X_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\n",
    "X_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of classes among sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "y = dict()\n",
    "y[0] = []\n",
    "y[1] = []\n",
    "for set_name in (y_train, y_val, y_test):\n",
    "    y[0].append(np.sum(set_name == 0))\n",
    "    y[1].append(np.sum(set_name == 1))\n",
    "\n",
    "trace0 = go.Bar(\n",
    "    x=['Train Set', 'Validation Set', 'Test Set'],\n",
    "    y=y[0],\n",
    "    name='No',\n",
    "    marker=dict(color='#33cc33'),\n",
    "    opacity=0.7\n",
    ")\n",
    "trace1 = go.Bar(\n",
    "    x=['Train Set', 'Validation Set', 'Test Set'],\n",
    "    y=y[1],\n",
    "    name='Yes',\n",
    "    marker=dict(color='#ff3300'),\n",
    "    opacity=0.7\n",
    ")\n",
    "data = [trace0, trace1]\n",
    "layout = go.Layout(\n",
    "    title='Count of classes in each set',\n",
    "    xaxis={'title': 'Set'},\n",
    "    yaxis={'title': 'Count'}\n",
    ")\n",
    "fig = go.Figure(data, layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def plot_samples(X, y, labels_dict, n=50):\n",
    "    \"\"\"\n",
    "    Creates a gridplot for desired number of images (n) from the specified set\n",
    "    \"\"\"\n",
    "    for index in range(len(labels_dict)):\n",
    "        imgs = X[np.argwhere(y == index)][:n]\n",
    "        j = 10\n",
    "        i = int(n/j)\n",
    "\n",
    "        plt.figure(figsize=(15,6))\n",
    "        c = 1\n",
    "        for img in imgs:\n",
    "            plt.subplot(i,j,c)\n",
    "            plt.imshow(img[0])\n",
    "\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            c += 1\n",
    "        plt.suptitle('Tumor: {}'.format(labels_dict[index]))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(X_train, y_train, labels, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images have different `width` and `height` and diffent size of \"black corners\". Since the image size for VGG-16 imput layer is `(224,224)` some wide images may look weird after resizing. Histogram of ratio distributions (`ratio = width/height`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "RATIO_LIST = []\n",
    "for set in (X_train, X_test, X_val):\n",
    "    for img in set:\n",
    "        RATIO_LIST.append(img.shape[1]/img.shape[0])\n",
    "plt.hist(RATIO_LIST)\n",
    "plt.title('Distribution of Image Ratios')\n",
    "plt.xlabel('Ratio Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropping the brain out of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def crop_imgs(set_name, add_pixels_value=0):\n",
    "    \"\"\"\n",
    "    Finds the extreme points on the image and crops the rectangular out of them\n",
    "    \"\"\"\n",
    "    set_new = []\n",
    "    for img in set_name:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "        # threshold the image, then perform a series of erosions +\n",
    "        # dilations to remove any small regions of noise\n",
    "        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "        thresh = cv2.erode(thresh, None, iterations=2)\n",
    "        thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "        # find contours in thresholded image, then grab the largest one\n",
    "        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = imutils.grab_contours(cnts)\n",
    "        c = max(cnts, key=cv2.contourArea)\n",
    "\n",
    "        # find the extreme points\n",
    "        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
    "        extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
    "        extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
    "        extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "\n",
    "        ADD_PIXELS = add_pixels_value\n",
    "        new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n",
    "        set_new.append(new_img)\n",
    "\n",
    "    return np.array(set_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('../input/brain-mri-images-for-brain-tumor-detection/brain_tumor_dataset/yes/Y108.jpg')\n",
    "img = cv2.resize(\n",
    "            img,\n",
    "            dsize=IMG_SIZE,\n",
    "            interpolation=cv2.INTER_CUBIC\n",
    "        )\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "# threshold the image, then perform a series of erosions +\n",
    "# dilations to remove any small regions of noise\n",
    "thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "thresh = cv2.erode(thresh, None, iterations=2)\n",
    "thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "# find contours in thresholded image, then grab the largest one\n",
    "cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = imutils.grab_contours(cnts)\n",
    "c = max(cnts, key=cv2.contourArea)\n",
    "\n",
    "# find the extreme points\n",
    "extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
    "extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
    "extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
    "extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "\n",
    "# add contour on the image\n",
    "img_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n",
    "\n",
    "# add extreme points\n",
    "img_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\n",
    "img_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\n",
    "img_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\n",
    "img_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n",
    "\n",
    "# crop\n",
    "ADD_PIXELS = 0\n",
    "new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(141)\n",
    "plt.imshow(img)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Step 1. Get the original image')\n",
    "plt.subplot(142)\n",
    "plt.imshow(img_cnt)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Step 2. Find the biggest contour')\n",
    "plt.subplot(143)\n",
    "plt.imshow(img_pnt)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Step 3. Find the extreme points')\n",
    "plt.subplot(144)\n",
    "plt.imshow(new_img)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Step 4. Crop the image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply this for each set\n",
    "X_train_crop = crop_imgs(set_name=X_train)\n",
    "X_val_crop = crop_imgs(set_name=X_val)\n",
    "X_test_crop = crop_imgs(set_name=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(X_train_crop, y_train, labels, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def save_new_images(x_set, y_set, folder_name):\n",
    "    i = 0\n",
    "    for (img, imclass) in zip(x_set, y_set):\n",
    "        if imclass == 0:\n",
    "            cv2.imwrite(folder_name+'NO/'+str(i)+'.jpg', img)\n",
    "        else:\n",
    "            cv2.imwrite(folder_name+'YES/'+str(i)+'.jpg', img)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving new images to the folder\n",
    "!mkdir TRAIN_CROP TEST_CROP VAL_CROP TRAIN_CROP/YES TRAIN_CROP/NO TEST_CROP/YES TEST_CROP/NO VAL_CROP/YES VAL_CROP/NO\n",
    "\n",
    "save_new_images(X_train_crop, y_train, folder_name='TRAIN_CROP/')\n",
    "save_new_images(X_val_crop, y_val, folder_name='VAL_CROP/')\n",
    "save_new_images(X_test_crop, y_test, folder_name='TEST_CROP/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing images to `(224,224)` and applying preprocessing needed for VGG-16 model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def preprocess_imgs(set_name, img_size):\n",
    "    \"\"\"\n",
    "    Resize and apply VGG-15 preprocessing\n",
    "    \"\"\"\n",
    "    set_new = []\n",
    "    for img in set_name:\n",
    "        img = cv2.resize(\n",
    "            img,\n",
    "            dsize=img_size,\n",
    "            interpolation=cv2.INTER_CUBIC\n",
    "        )\n",
    "        set_new.append(preprocess_input(img))\n",
    "    return np.array(set_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep = preprocess_imgs(set_name=X_train_crop, img_size=IMG_SIZE)\n",
    "X_test_prep = preprocess_imgs(set_name=X_test_crop, img_size=IMG_SIZE)\n",
    "X_val_prep = preprocess_imgs(set_name=X_val_crop, img_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(X_train_prep, y_train, labels, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Convolutional Neural Network (CNN) Model Architecture\n",
    "\n",
    "Using Transfer Learning Approach with VGG-16 architecture and weights as a base model. The VGG-16 CNN Model is pre-trained on `imagenet`\n",
    "\n",
    "### 4.1. Data Augmentation\n",
    "\n",
    "Data Augmentation is used which helps to \"increase\" the size of training set since limited training data is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the paramters we want to change randomly\n",
    "demo_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.05,\n",
    "    brightness_range=[0.1, 1.5],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "os.mkdir('preview')\n",
    "x = X_train_crop[0]  \n",
    "x = x.reshape((1,) + x.shape) \n",
    "\n",
    "i = 0\n",
    "for batch in demo_datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='aug_img', save_format='jpg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_train_crop[0])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Original Image')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "i = 1\n",
    "for img in os.listdir('preview/'):\n",
    "    img = cv2.cv2.imread('preview/' + img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.subplot(3,7,i)\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    i += 1\n",
    "    if i > 3*7:\n",
    "        break\n",
    "plt.suptitle('Augemented Images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "!rm -rf preview/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'TRAIN_CROP/'\n",
    "VAL_DIR = 'VAL_CROP/'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    color_mode='rgb',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    color_mode='rgb',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Building the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model\n",
    "vgg16_weight_path = '../input/keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "base_model = VGG16(\n",
    "    weights=vgg16_weight_path,\n",
    "    include_top=False, \n",
    "    input_shape=IMG_SIZE + (3,)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=RMSprop(lr=1e-4),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "es = EarlyStopping(\n",
    "    monitor='val_acc', \n",
    "    mode='max',\n",
    "    patience=6\n",
    ")\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=50,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=25,\n",
    "    callbacks=[es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Model Performance Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# plot model performance\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(1, len(history.epoch) + 1)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Train Set')\n",
    "plt.plot(epochs_range, val_acc, label='Val Set')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Train Set')\n",
    "plt.plot(epochs_range, val_loss, label='Val Set')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate on val set\n",
    "predictions = model.predict(X_val_prep)\n",
    "predictions = [1 if x>0.5 else 0 for x in predictions]\n",
    "\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print('Val Accuracy = %.2f' % accuracy)\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_val, predictions) \n",
    "cm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate on test set\n",
    "predictions = model.predict(X_test_prep)\n",
    "predictions = [1 if x>0.5 else 0 for x in predictions]\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('Test Accuracy = %.2f' % accuracy)\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_test, predictions) \n",
    "cm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images from the `test set` that were misclassified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "ind_list = np.argwhere((y_test == predictions) == False)[:, -1]\n",
    "if ind_list.size == 0:\n",
    "    print('There are no missclassified images.')\n",
    "else:\n",
    "    for i in ind_list:\n",
    "        plt.figure()\n",
    "        plt.imshow(X_test_crop[i])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(f'Actual class: {y_val[i]}\\nPredicted class: {predictions[i]}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "We used Convolutional Neural Network to predict whether the subject has Brain Tumor or not from MRI Images. Methods that could be used to increase accuracy includes, using large no. of images i.e., a larger dataset, Hyperparameter Tuning, Using a different Convolutional Neural Network Model may also result in higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
